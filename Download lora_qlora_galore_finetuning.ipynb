{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b60cc6",
   "metadata": {},
   "source": [
    "# LoRA vs QLoRA vs GaLore Fine-Tuning: Practical Colab Notebook\n",
    "\n",
    "This notebook lets you:\n",
    "- Fine-tune a small LLM with **LoRA**\n",
    "- Fine-tune with **QLoRA** using quantization for low VRAM\n",
    "- Apply **GaLore** for efficient full-model fine-tuning\n",
    "- Track VRAM usage and training speed\n",
    "\n",
    "ðŸ‘‹ Created for **SuperML.dev** readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2815cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers peft bitsandbytes galore-torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da45ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ce4bc",
   "metadata": {},
   "source": [
    "## ðŸ”¹ LoRA Fine-Tuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tiiuae/falcon-7b-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16, lora_dropout=0.05)\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "model.train()\n",
    "print('LoRA applied. Model ready for training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac08872",
   "metadata": {},
   "source": [
    "## ðŸ”¹ QLoRA Fine-Tuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6dbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map='auto')\n",
    "\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16, lora_dropout=0.05)\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "model.train()\n",
    "print('QLoRA applied. Model ready for training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e207bd0",
   "metadata": {},
   "source": [
    "## ðŸ”¹ GaLore Full-Model Fine-Tuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galore_torch import GaLoreAdamW\n",
    "\n",
    "optimizer = GaLoreAdamW(model.parameters(), lr=2e-5, weight_decay=0.01, rank=32, update_proj_gap=200)\n",
    "\n",
    "# Example dummy forward + backward\n",
    "input_ids = tokenizer('Hello world', return_tensors='pt').input_ids.to(model.device)\n",
    "outputs = model(input_ids, labels=input_ids)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('GaLore optimization step completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d482e1",
   "metadata": {},
   "source": [
    "## ðŸ”¹ GPU VRAM Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d93a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad21af",
   "metadata": {},
   "source": [
    "## âœ… Done\n",
    "\n",
    "- You have now tested **LoRA**, **QLoRA**, and **GaLore** fine-tuning setups.\n",
    "- Adapt these cells with your dataset and training loops as needed.\n",
    "\n",
    "ðŸ‘‹ Share your experiments on [r/supermldev](https://reddit.com/r/supermldev)!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
